#!/usr/bin/env python3
"""
Test Dynamic Remediation Engine (LLM-driven, no templates)
"""

import sys
import time
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

import yaml
from loguru import logger

# Configure logging
logger.remove()
logger.add(sys.stdout, format="<green>{time:HH:mm:ss}</green> | {message}", level="INFO")

def main():
    print("=" * 80)
    print("TESTING DYNAMIC REMEDIATION ENGINE (LLM-Driven)")
    print("=" * 80)
    print()
    
    # Load config
    config_path = Path(__file__).parent.parent / 'config.yaml'
    with open(config_path) as f:
        config = yaml.safe_load(f)
    
    start_time = time.time()
    
    # Initialize system
    print("üîß Initializing system...")
    from violation_finder.violation_finder import GDPRViolationFinder
    finder = GDPRViolationFinder(config)
    init_time = time.time() - start_time
    print(f"‚úÖ Initialized in {init_time:.2f}s")
    print()
    
    # Test scenario
    scenario = """
    Our mobile app collects user location data every 5 minutes without asking permission.
    We use this data to build advertising profiles and sell these profiles to data brokers.
    Users cannot see what data we collected or delete it.
    We don't have a privacy policy.
    """
    
    print("üìÑ SCENARIO:")
    print(scenario)
    print()
    print("-" * 80)
    print()
    
    # Analyze
    print("üîç Analyzing with DYNAMIC LLM-driven remediation...")
    print()
    
    analysis_start = time.time()
    assessment = finder.analyze_scenario(scenario)
    analysis_time = time.time() - analysis_start
    
    print(f"‚úÖ Analysis completed in {analysis_time:.2f}s")
    print()
    
    # Results
    print("=" * 80)
    print(f"üìä RESULTS:")
    print(f"   Risk Level: {assessment.overall_risk_level} ({assessment.risk_score:.1f}/10)")
    print(f"   Violations Found: {len(assessment.violations)}")
    print("=" * 80)
    print()
    
    # Show first violation with remediation
    if assessment.violations:
        v = assessment.violations[0]
        print(f"### VIOLATION 1: {v.category}")
        print(f"Severity: {v.severity}")
        print(f"Articles: {', '.join(v.articles)}")
        print()
        
        if v.remediation_guidance:
            rem = v.remediation_guidance
            print("üîß DYNAMIC REMEDIATION (Generated by LLM):")
            print(f"  Priority: {rem.priority.value}")
            print(f"  Complexity: {rem.complexity.value}")
            print(f"  Cost: {rem.estimated_cost_range}")
            print(f"  Effort: {rem.estimated_effort}")
            print()
            
            print("  üö® Immediate Actions:")
            for action in rem.immediate_actions[:3]:
                print(f"    - {action}")
            print()
            
            print("  üìÖ Short-Term Solutions:")
            for solution in rem.short_term_solutions[:3]:
                print(f"    - {solution}")
            print()
            
            if rem.verification_checklist:
                print("  ‚úÖ Verification Checklist:")
                for check in rem.verification_checklist[:3]:
                    print(f"    {check}")
                print()
        else:
            print("‚ö†Ô∏è No remediation generated")
        print()
    
    # Check if guidance is specific (not generic)
    print("=" * 80)
    print("VERIFICATION:")
    print("=" * 80)
    
    if assessment.violations and assessment.violations[0].remediation_guidance:
        rem = assessment.violations[0].remediation_guidance
        actions_text = " ".join(rem.immediate_actions).lower()
        
        # Check if it's specific (not generic fallback)
        is_generic = "conduct detailed compliance assessment" in actions_text
        
        if not is_generic:
            print("‚úÖ Remediation is SPECIFIC (LLM-generated, not fallback)")
        else:
            print("‚ö†Ô∏è Using generic fallback (LLM may have failed)")
        
        # Check for context-awareness
        if "location" in actions_text or "consent" in actions_text or "privacy policy" in actions_text:
            print("‚úÖ Remediation is CONTEXT-AWARE (mentions scenario details)")
        else:
            print("‚ö†Ô∏è May not be fully context-aware")
    
    print()
    print("=" * 80)
    print(f"‚è±Ô∏è  TOTAL TIME: {time.time() - start_time:.2f}s")
    print("=" * 80)
    
    # Generate report
    print()
    print("üìÑ GENERATING REPORT...")
    report = finder.generate_compliance_report(scenario, assessment, format='markdown')
    
    output_path = Path(__file__).parent.parent / 'logs' / 'dynamic_remediation_test_report.md'
    output_path.parent.mkdir(exist_ok=True)
    
    with open(output_path, 'w') as f:
        f.write(report)
    
    print(f"üìÅ Saved to: {output_path}")
    print()
    print("‚úÖ TEST COMPLETE")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è Test interrupted by user")
    except Exception as e:
        print(f"\n\n‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()
