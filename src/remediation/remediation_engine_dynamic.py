"""
Dynamic Remediation Engine - LLM-Driven GDPR Compliance Solutions
Uses RAG + LLM to generate specific, context-aware remediation guidance
NO hardcoded templates - fully dynamic based on violation context
"""
from dataclasses import dataclass
from typing import List, Dict, Optional
from enum import Enum
import logging

logger = logging.getLogger(__name__)


class RemediationPriority(Enum):
    """Priority levels for remediation actions"""
    CRITICAL = "Critical"
    HIGH = "High"
    MEDIUM = "Medium"
    LOW = "Low"


class RemediationComplexity(Enum):
    """Implementation complexity"""
    SIMPLE = "Simple"
    MODERATE = "Moderate"
    COMPLEX = "Complex"
    MAJOR = "Major"


@dataclass
class RemediationStep:
    """Single remediation step with details"""
    step_number: int
    action: str
    owner: str
    timeline: str
    success_criteria: str
    resources_needed: List[str]


@dataclass
class RemediationGuidance:
    """Comprehensive remediation guidance for a violation"""
    violation_category: str
    affected_articles: List[str]
    priority: RemediationPriority
    complexity: RemediationComplexity
    estimated_effort: str
    immediate_actions: List[str]
    short_term_solutions: List[str]
    long_term_improvements: List[str]
    detailed_steps: List[RemediationStep]
    technical_requirements: List[str]
    policy_requirements: List[str]
    training_requirements: List[str]
    estimated_cost_range: str
    required_roles: List[str]
    verification_checklist: List[str]
    documentation_needed: List[str]
    ongoing_monitoring: str
    best_practices: List[str]
    helpful_resources: List[str]
    similar_cases: List[str]


class DynamicRemediationEngine:
    """
    LLM-driven remediation guidance engine
    Generates specific, context-aware solutions using RAG + dynamic prompts
    """
    
    def __init__(self, rag_system=None, llm_client=None):
        """
        Initialize dynamic remediation engine
        
        Args:
            rag_system: RAG system for retrieving GDPR context
            llm_client: LLM client for generating guidance
        """
        self.rag_system = rag_system
        self.llm_client = llm_client
        logger.info("Initialized Dynamic Remediation Engine (LLM-driven)")
    
    def generate_remediation(
        self,
        violation_category: str,
        articles: List[str],
        severity: str,
        context: str,
        evidence: str = "",
        all_violations_text: str = ""
    ) -> RemediationGuidance:
        """
        Generate dynamic, context-aware remediation using LLM
        Uses a two-step process: violation analysis already done, now generate remediation
        
        Args:
            violation_category: Type of violation
            articles: Affected GDPR articles
            severity: Violation severity
            context: Scenario context
            evidence: Problematic text from scenario
            all_violations_text: Full text of all violations found (for context)
        
        Returns:
            Complete remediation guidance generated by LLM
        """
        try:
            # Step 1: Retrieve relevant GDPR context from RAG
            gdpr_context = self._retrieve_gdpr_context(articles, violation_category)
            
            # Step 2: Build focused remediation prompt (separate from violation finding)
            prompt = self._build_remediation_prompt(
                violation_category=violation_category,
                articles=articles,
                severity=severity,
                context=context,
                evidence=evidence,
                gdpr_context=gdpr_context,
                all_violations_text=all_violations_text
            )
            
            # Step 3: Generate remediation using LLM (second LLM call)
            llm_response = self._generate_with_llm(prompt)
            
            # Step 4: Parse LLM response into structured format
            remediation = self._parse_llm_response(
                llm_response,
                violation_category,
                articles,
                severity
            )
            
            return remediation
            
        except Exception as e:
            logger.warning(f"Error generating dynamic remediation: {e}")
            return self._generate_fallback_remediation(violation_category, articles, severity)
    
    def _retrieve_gdpr_context(self, articles: List[str], category: str) -> str:
        """Retrieve relevant GDPR articles and guidance from RAG"""
        if not self.rag_system:
            return ""
        
        try:
            # Build query for remediation-specific GDPR content
            query_parts = []
            if articles:
                query_parts.append(f"Articles: {', '.join(articles)}")
            query_parts.append(f"Category: {category}")
            query_parts.append("compliance requirements remediation solutions")
            
            query = " ".join(query_parts)
            
            # Retrieve from RAG (fewer results, focused on remediation)
            results = self.rag_system.retrieve(query, top_k=3)
            
            # Combine retrieved context
            context_parts = []
            for i, result in enumerate(results, 1):
                context_parts.append(f"GDPR Reference {i}:")
                context_parts.append(result.get('text', ''))
                context_parts.append("")
            
            return "\n".join(context_parts)
            
        except Exception as e:
            logger.warning(f"Error retrieving GDPR context: {e}")
            return ""
    
    def _build_remediation_prompt(
        self,
        violation_category: str,
        articles: List[str],
        severity: str,
        context: str,
        evidence: str,
        gdpr_context: str,
        all_violations_text: str = ""
    ) -> str:
        """Build specialized remediation prompt using template from config"""
        
        # Get prompt template from config
        from yaml import safe_load
        try:
            with open('config.yaml', 'r') as f:
                config = safe_load(f)
                prompt_template = config.get('prompts', {}).get('remediation_generator_prompt', '')
        except:
            # Fallback if config not accessible
            prompt_template = """Generate remediation for {violation_category}.
            
            Articles: {articles}
            Severity: {severity}
            
            GDPR Context: {context}
            
            Return JSON with: immediate_actions, short_term, long_term, estimated_cost, estimated_effort, key_roles."""
        
        articles_str = ", ".join(articles) if articles else "General GDPR"
        
        # Replace placeholders
        prompt = prompt_template.replace('{violation_category}', violation_category)
        prompt = prompt.replace('{articles}', articles_str)
        prompt = prompt.replace('{severity}', severity)
        prompt = prompt.replace('{context}', gdpr_context[:1000] if gdpr_context else "")
        prompt = prompt.replace('{scenario}', context[:500] if context else "")
        prompt = prompt.replace('{violations}', all_violations_text[:500] if all_violations_text else f"{violation_category} - {evidence[:200]}")
        
        return prompt
    
    def _generate_with_llm(self, prompt: str) -> str:
        """Generate remediation using LLM"""
        if not self.llm_client:
            raise Exception("LLM client not available")
        
        try:
            response = self.llm_client.generate(
                prompt=prompt,
                temperature=0.3,  # Lower temperature for more consistent output
                max_tokens=2000
            )
            return response
        except Exception as e:
            logger.error(f"LLM generation failed: {e}")
            raise
    
    def _parse_llm_response(
        self,
        llm_response: str,
        violation_category: str,
        articles: List[str],
        severity: str
    ) -> RemediationGuidance:
        """Parse simplified JSON response into RemediationGuidance"""
        import json
        import re
        
        try:
            # Clean response - remove markdown if present
            cleaned = llm_response.strip()
            if '```json' in cleaned:
                match = re.search(r'```json\s*(\{.*?\})\s*```', cleaned, re.DOTALL)
                if match:
                    cleaned = match.group(1)
            elif '```' in cleaned:
                match = re.search(r'```\s*(\{.*?\})\s*```', cleaned, re.DOTALL)
                if match:
                    cleaned = match.group(1)
            
            # Try to fix incomplete JSON
            if not cleaned.endswith('}'):
                logger.warning(f"JSON appears incomplete (length: {len(cleaned)})")
                # Add missing closing braces
                open_braces = cleaned.count('{') - cleaned.count('}')
                if open_braces > 0:
                    cleaned += '}' * open_braces
                open_brackets = cleaned.count('[') - cleaned.count(']')
                if open_brackets > 0:
                    cleaned += ']' * open_brackets
            
            # Parse JSON
            data = json.loads(cleaned)
            logger.info(f"Successfully parsed remediation JSON")
            
            # Map severity to priority
            priority_map = {
                'Critical': RemediationPriority.CRITICAL,
                'High': RemediationPriority.HIGH,
                'Medium': RemediationPriority.MEDIUM,
                'Low': RemediationPriority.LOW
            }
            priority = priority_map.get(severity, RemediationPriority.MEDIUM)
            
            # Create minimal implementation steps from immediate actions
            steps = []
            for idx, action in enumerate(data.get('immediate_actions', [])[:3], 1):
                steps.append(RemediationStep(
                    step_number=idx,
                    action=action,
                    owner=data.get('key_roles', ['Legal', 'IT'])[0] if data.get('key_roles') else 'Compliance',
                    timeline='1 week',
                    success_criteria=f'{action[:50]} completed',
                    resources_needed=['Internal resources']
                ))
            
            return RemediationGuidance(
                violation_category=violation_category,
                affected_articles=articles,
                priority=priority,
                complexity=RemediationComplexity.MODERATE,
                estimated_effort=data.get('estimated_effort', '2-4 weeks'),
                immediate_actions=data.get('immediate_actions', ['Consult legal counsel']),
                short_term_solutions=data.get('short_term', ['Implement controls']),
                long_term_improvements=data.get('long_term', ['Monitor compliance']),
                detailed_steps=steps,
                technical_requirements=data.get('technical_requirements', []),
                policy_requirements=data.get('policy_requirements', []),
                training_requirements=data.get('training_requirements', []),
                estimated_cost_range=data.get('estimated_cost', '$10k-$30k'),
                required_roles=data.get('key_roles', ['Legal', 'IT', 'Compliance']),
                verification_checklist=data.get('verification_checklist', []),
                documentation_needed=data.get('documentation_needed', []),
                ongoing_monitoring=data.get('ongoing_monitoring', 'Regular audits'),
                best_practices=data.get('best_practices', []),
                helpful_resources=data.get('resources', []),
                similar_cases=data.get('similar_cases', [])
            )
            
        except (json.JSONDecodeError, Exception) as e:
            logger.warning(f"Failed to parse remediation JSON: {e}")
            logger.warning(f"Response length: {len(llm_response)} chars, preview: {llm_response[:300]}")
            return self._create_fallback_remediation(violation_category, articles, severity)
    
    def _create_fallback_remediation(
        self,
        violation_category: str,
        articles: List[str],
        severity: str
    ) -> RemediationGuidance:
        """Create basic fallback remediation"""
        priority_map = {
            'Critical': RemediationPriority.CRITICAL,
            'High': RemediationPriority.HIGH,
            'Medium': RemediationPriority.MEDIUM,
            'Low': RemediationPriority.LOW
        }
        
        return RemediationGuidance(
            violation_category=violation_category,
            affected_articles=articles,
            priority=priority_map.get(severity, RemediationPriority.MEDIUM),
            complexity=RemediationComplexity.MODERATE,
            estimated_effort='2-4 weeks',
            immediate_actions=['Conduct compliance audit', 'Consult legal counsel'],
            short_term_solutions=['Implement necessary controls'],
            long_term_improvements=['Establish monitoring'],
            detailed_steps=[],
            technical_requirements=[],
            policy_requirements=[],
            training_requirements=[],
            estimated_cost_range='$10k-$30k',
            required_roles=['Legal', 'IT', 'Compliance'],
            verification_checklist=[],
            documentation_needed=[],
            ongoing_monitoring='Regular compliance audits',
            best_practices=[],
            helpful_resources=[],
            similar_cases=[]
        )
    
    def _parse_text_fallback(
        self,
        llm_response: str,
        violation_category: str,
        articles: List[str],
        severity: str
    ) -> RemediationGuidance:
        """Fallback text parser for non-JSON responses"""
        
        # Extract priority
        priority = self._extract_priority(llm_response, severity)
        
        # Extract complexity
        complexity = self._extract_complexity(llm_response)
        
        # Extract effort and cost
        effort = self._extract_field(llm_response, "EFFORT:")
        cost = self._extract_field(llm_response, "COST:")
        
        # Extract lists
        immediate = self._extract_list_items(llm_response, "IMMEDIATE_ACTIONS", "SHORT_TERM")
        short_term = self._extract_list_items(llm_response, "SHORT_TERM", "LONG_TERM")
        long_term = self._extract_list_items(llm_response, "LONG_TERM", "IMPLEMENTATION_STEPS")
        
        # Extract implementation steps
        steps = self._extract_implementation_steps(llm_response)
        
        # Extract requirements
        technical_req = self._extract_list_items(llm_response, "TECHNICAL_REQUIREMENTS:", "POLICY_REQUIREMENTS")
        policy_req = self._extract_list_items(llm_response, "POLICY_REQUIREMENTS:", "TRAINING_REQUIREMENTS")
        training_req = self._extract_list_items(llm_response, "TRAINING_REQUIREMENTS:", "VERIFICATION_CHECKLIST")
        
        # Extract verification checklist
        verification = self._extract_list_items(llm_response, "VERIFICATION_CHECKLIST:", "DOCUMENTATION")
        
        # Extract documentation
        documentation = self._extract_list_items(llm_response, "DOCUMENTATION:", "MONITORING")
        
        # Extract monitoring
        monitoring = self._extract_field(llm_response, "MONITORING:", stop_at=["BEST_PRACTICES"])
        
        # Extract best practices
        best_practices = self._extract_list_items(llm_response, "BEST_PRACTICES:", "RESOURCES")
        
        # Extract resources
        resources = self._extract_list_items(llm_response, "RESOURCES:", "SIMILAR_CASES")
        
        # Extract similar cases
        similar_cases = self._extract_list_items(llm_response, "SIMILAR_CASES:", "ROLES_NEEDED")
        
        # Extract roles
        roles = self._extract_roles(llm_response)
        
        return RemediationGuidance(
            violation_category=violation_category,
            affected_articles=articles,
            priority=priority,
            complexity=complexity,
            estimated_effort=effort or "2-4 weeks",
            immediate_actions=immediate or ["Consult legal counsel", "Audit current practices"],
            short_term_solutions=short_term or ["Implement compliance controls"],
            long_term_improvements=long_term or ["Continuous monitoring"],
            detailed_steps=steps,
            technical_requirements=technical_req or [],
            policy_requirements=policy_req or [],
            training_requirements=training_req or [],
            estimated_cost_range=cost or "$10k-$30k",
            required_roles=roles or ["Legal", "IT", "Compliance"],
            verification_checklist=verification or [],
            documentation_needed=documentation or [],
            ongoing_monitoring=monitoring or "Regular compliance audits",
            best_practices=best_practices or [],
            helpful_resources=resources or [],
            similar_cases=similar_cases or []
        )
    
    def _extract_priority(self, text: str, severity: str) -> RemediationPriority:
        """Extract priority from LLM response or derive from severity"""
        text_upper = text.upper()
        if "PRIORITY: CRITICAL" in text_upper:
            return RemediationPriority.CRITICAL
        elif "PRIORITY: HIGH" in text_upper:
            return RemediationPriority.HIGH
        elif "PRIORITY: MEDIUM" in text_upper:
            return RemediationPriority.MEDIUM
        elif "PRIORITY: LOW" in text_upper:
            return RemediationPriority.LOW
        
        # Fallback based on severity
        severity_lower = severity.lower()
        if severity_lower == "critical":
            return RemediationPriority.CRITICAL
        elif severity_lower == "high":
            return RemediationPriority.HIGH
        else:
            return RemediationPriority.MEDIUM
    
    def _extract_complexity(self, text: str) -> RemediationComplexity:
        """Extract complexity from LLM response"""
        text_upper = text.upper()
        if "COMPLEXITY: SIMPLE" in text_upper:
            return RemediationComplexity.SIMPLE
        elif "COMPLEXITY: MODERATE" in text_upper:
            return RemediationComplexity.MODERATE
        elif "COMPLEXITY: COMPLEX" in text_upper:
            return RemediationComplexity.COMPLEX
        elif "COMPLEXITY: MAJOR" in text_upper:
            return RemediationComplexity.MAJOR
        return RemediationComplexity.MODERATE
    
    def _extract_field(self, text: str, marker: str, stop_at: List[str] = None) -> str:
        """Extract a single field value"""
        try:
            start_idx = text.find(marker)
            if start_idx == -1:
                return ""
            
            start_idx += len(marker)
            end_idx = text.find("\n", start_idx)
            
            if end_idx == -1:
                end_idx = len(text)
            
            value = text[start_idx:end_idx].strip()
            return value
        except:
            return ""
    
    def _extract_list_items(self, text: str, start_marker: str, end_marker: str) -> List[str]:
        """Extract list items between two markers"""
        try:
            start_idx = text.find(start_marker)
            if start_idx == -1:
                return []
            
            start_idx += len(start_marker)
            end_idx = text.find(end_marker, start_idx)
            
            if end_idx == -1:
                end_idx = len(text)
            
            section = text[start_idx:end_idx].strip()
            
            # Extract lines starting with - or ☐
            items = []
            for line in section.split("\n"):
                line = line.strip()
                if line.startswith("-") or line.startswith("☐"):
                    item = line.lstrip("-☐ ").strip()
                    if item:
                        items.append(item)
            
            return items
        except:
            return []
    
    def _extract_implementation_steps(self, text: str) -> List[RemediationStep]:
        """Extract implementation steps from LLM response"""
        steps = []
        try:
            start_idx = text.find("IMPLEMENTATION_STEPS:")
            if start_idx == -1:
                return []
            
            end_idx = text.find("TECHNICAL_REQUIREMENTS:", start_idx)
            if end_idx == -1:
                end_idx = len(text)
            
            section = text[start_idx:end_idx]
            
            step_num = 1
            for line in section.split("\n"):
                if line.strip().startswith("Step"):
                    parts = line.split("|")
                    if len(parts) >= 5:
                        action = parts[0].split(":", 1)[1].strip() if ":" in parts[0] else parts[0].strip()
                        owner = parts[1].replace("Owner:", "").strip()
                        timeline = parts[2].replace("Timeline:", "").strip()
                        success = parts[3].replace("Success:", "").strip()
                        resources = parts[4].replace("Resources:", "").strip().split(",")
                        resources = [r.strip() for r in resources]
                        
                        steps.append(RemediationStep(
                            step_number=step_num,
                            action=action,
                            owner=owner,
                            timeline=timeline,
                            success_criteria=success,
                            resources_needed=resources
                        ))
                        step_num += 1
        except Exception as e:
            logger.warning(f"Error parsing implementation steps: {e}")
        
        return steps
    
    def _extract_roles(self, text: str) -> List[str]:
        """Extract required roles from LLM response"""
        try:
            start_idx = text.find("ROLES_NEEDED:")
            if start_idx == -1:
                return []
            
            start_idx += len("ROLES_NEEDED:")
            section = text[start_idx:start_idx + 200]  # Take next 200 chars
            
            # Split by common delimiters
            roles = []
            for delimiter in [",", ";", "\n"]:
                if delimiter in section:
                    parts = section.split(delimiter)
                    roles = [r.strip() for r in parts if r.strip()]
                    break
            
            return roles[:10]  # Limit to 10 roles
        except:
            return []
    
    def _generate_fallback_remediation(
        self,
        violation_category: str,
        articles: List[str],
        severity: str
    ) -> RemediationGuidance:
        """Generate minimal fallback remediation if LLM fails"""
        
        priority = RemediationPriority.HIGH
        if severity.lower() == "critical":
            priority = RemediationPriority.CRITICAL
        
        return RemediationGuidance(
            violation_category=violation_category,
            affected_articles=articles,
            priority=priority,
            complexity=RemediationComplexity.MODERATE,
            estimated_effort="2-4 weeks",
            immediate_actions=[
                "🔍 Conduct detailed compliance assessment",
                "⚖️ Consult with legal counsel",
                "📊 Audit affected systems and processes",
            ],
            short_term_solutions=[
                "🔧 Implement technical controls",
                "📄 Update policies and procedures",
                "🎓 Train staff on requirements",
            ],
            long_term_improvements=[
                "📈 Ongoing monitoring and auditing",
                "🔄 Continuous improvement program",
            ],
            detailed_steps=[],
            technical_requirements=[],
            policy_requirements=[],
            training_requirements=[],
            estimated_cost_range="$10k-$30k",
            required_roles=["Legal", "IT", "Compliance"],
            verification_checklist=[],
            documentation_needed=[],
            ongoing_monitoring="Regular compliance audits",
            best_practices=[],
            helpful_resources=[],
            similar_cases=[]
        )
